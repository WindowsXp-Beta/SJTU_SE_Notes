# pytorch

[60 minutes tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)

## Tensor

> similiar to numpy ndarray, but can be accelerated on GPU or other hardware

### initialize

```python
data = [[1, 2], [3, 4]]
# directly from data
x_data = torch.tensor(data)
# from numpy array
np_array = np.array(data)
x_np = torch.from_numpy(np_array)
# from another tensor
x_ones = torch.ones_like(x_data) # retains the properties of x_data
print(f"Ones Tensor: \n {x_ones} \n")

x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data
print(f"Random Tensor: \n {x_rand} \n")
```

### attribute

```python
tensor = torch.rand(3, 4)

print(f"Shape of tensor: {tensor.shape}")# shape
print(f"Datatype of tensor: {tensor.dtype}")# data type 
print(f"Device tensor is stored on: {tensor.device}")# CPU, GPU, etc
```

### operation

go to official doc

## torch.autograd

> `torch.autograd` is PyTorch’s automatic differentiation engine that powers neural network training.

Training a NN happens in two steps:

1. Forward Propagation： 将数据输入神经网络得到预测结果与损失
2. Backward Propagation：反向传播优化网络。

```python
import torch, torchvision
model = torchvision.models.resnet18(pretrained=True) # pre-trained resNet
data = torch.rand(1, 3, 64, 64)
labels = torch.rand(1, 1000)

prediction = model(data) # forward pass

loss = (prediction - labels).sum()
loss.backward() # backward pass
```

## Neural Network

An `nn.Module` contains layers, and a method `forward(input)` that returns the `output`.

A typical training procedure for a neural network is as follows:

- 定义神经网络 Define the neural network that has some learnable parameters (or weights)
- 遍历数据集 Iterate over a dataset of inputs
- 处理数据 Process input through the network
- 计算损失 Compute the loss (how far is the output from being correct)
- 反向传播 Propagate gradients back into the network’s parameters
- 更新参数 Update the weights of the network, typically using a simple update rule: `weight = weight - learning_rate * gradient`

an example of LenNet

![](./note_img/mnist.png)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 5)
        # 1 input image channel(For 3 channel RGB, it should be 3)
        # 6 output channels(The number of kernels)
        # 5x5 square convolution
		# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)

        self.conv2 = nn.Conv2d(6, 16, 5)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension
        # torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)
        # Applies a linear transformation to the incoming data
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
        # stride default value is kernel size
        # If the size is a square, you can specify with a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension
        # torch.flatten(input, start_dim=0, end_dim=- 1)
        # only dimensions starting with start_dim and ending with end_dim are flattened.
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()
print(net)
```

在Net类中定义了forward function，backword function当使用autograd时会自动定义

神经网络尺寸变化：

$N=(W-F+2P)/S+1$

可以用`net.parameters()`查看网络的参数，对于LenNet，结果如下：

```python
params = list(net.parameters())
print(len(params))
for i in params:
	print(i.size())
```

```python
10
torch.Size([6, 1, 5, 5]) # 第一个卷积层
torch.Size([6]) # bias
torch.Size([16, 6, 5, 5])
torch.Size([16])
torch.Size([120, 400]) # 第一个全连接层 y=A^Tx+b y=(120,1) x=(16 * 5 * 5,1) so A^T=(120,400)
torch.Size([120]) # bias shape is same as y (120, 1)
torch.Size([84, 120])
torch.Size([84])
torch.Size([10, 84])
torch.Size([10])
```

### loss

A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.

when we call `loss.backward()`, the whole graph is differentiated

### Backprop

To backpropagate the error all we have to do is to `loss.backward()`. 

>  You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.

```python
net.zero_grad()     # zeroes the gradient buffers of all parameters

print('conv1.bias.grad before backward')
print(net.conv1.bias.grad)

loss.backward()

print('conv1.bias.grad after backward')
print(net.conv1.bias.grad)
```

### Update the weights

Simplest rule：Stochastic Gradient Descent (SGD): 随机梯度下降

`weight = weight - learning_rate * gradient`

你可以直接用一小段代码实现SGD：

```python
learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
```

However, you want to use various different update rules such SGD, Nesterov-SGD, Adam, RMSProp, etc.

`torch.optim` that implements all these methods. Using it is very simple:

```python
import torch.optim as optim

# create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01) # learning rate

# in your training loop:
optimizer.zero_grad()   # zero the gradient buffers
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()    # Does the update
```

# numpy

## 数组

> Q&A
>
> 1. ndarray vs array
>
>    ndarray是一个类，其默认构造函数是ndarray()。
>    array是一个函数，便于创建一个ndarray对象。

### 初始化

np.array() [0,1]随机值

np.zeros()

## 乘法

`@/np.matmul` 矩阵乘

[np.dot(a, b)](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)

1. for 2-D array dot is same as @
2. for 1-D array dot is inner product
3. either a or b is 0-D array(scalar) dot is same 数乘`numpy.multiply(a, b)`
4. If *a* is an N-D array and *b* is a 1-D array, it is a sum product over the last axis of *a* and *b*.

# matplotlib

## pyplot

[matplotlib绘图入门详解](https://www.jianshu.com/p/da385a35f68d)

[Python-matplotlib画图(莫烦笔记)](https://blog.csdn.net/gaotihong/article/details/80983937)